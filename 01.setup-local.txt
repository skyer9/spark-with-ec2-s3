
로컬환경 구성하기



Spark 서버 클러스터를 생성하기 전에 로컬(테스트용) 서버를 생성합니다.



1. Amazon Linux AMI 를 생성하고, SSD 용량은 20G 로 합니다.

2. 자바를 업그래이드한다.

   $ sudo yum -y install java-1.8.0
   $ sudo yum -y remove java-1.7.0-openjdk

3. 파이선 3.x 을 설치한다.

   $ sudo yum list | grep python3
   $ sudo yum install python36

4. Spark 를 설치한다.

   https://spark.apache.org/downloads.html 에서 다운받아 설치한다.

   $ cd ~/
   $ mkdir spark
   $ wget http://mirror.navercorp.com/apache/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz
   $ tar xvfz spark-2.3.0-bin-hadoop2.7.tgz

5. 아나콘다를 설치한다.

   $ wget https://repo.continuum.io/archive/Anaconda3-4.4.0-Linux-x86_64.sh
   $ sh Anaconda3-4.4.0-Linux-x86_64.sh

   로그아웃 후 재로그인한다.

6. pyspark 를 설치한다.

   $ conda install -c conda-forge pyspark

   $ vi ~/.bashrc
   ......
   export SPARK_HOME='/home/ec2-user/spark/spark-2.3.0-bin-hadoop2.7'
   export PATH="$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH"
   ......

   추가 jar 파일을 설치한다.

   $ wget http://central.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.5/hadoop-aws-2.7.5.jar
   $ cp hadoop-aws-2.7.5.jar ~/spark/spark-2.3.0-bin-hadoop2.7/jars/

   최신 jar 파일 대신 아래 버전을 다운받는다.

   $ wget http://central.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar
   $ cp aws-java-sdk-1.7.4.jar ~/spark/spark-2.3.0-bin-hadoop2.7/jars/

7. 정상 실행되는지 확인한다.

   $ cd spark-2.3.0-bin-hadoop2.7
   $ ./bin/spark-shell

   정상적으로 실행되면 :quit 을 입력해 종료한다.
