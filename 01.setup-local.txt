
로컬환경 구성하기



Spark 서버 클러스터를 생성하기 전에 로컬(테스트용) 서버를 생성합니다.

1. Amazon Linux AMI 를 생성하고, 인스턴스 유형은 t2.medium, SSD 용량은 20G 로 합니다.

   인스턴스가 생성되면 아래 명령을 실행해준다.

   $ sudo yum -y update

2. 자바를 업그래이드한다.

   $ sudo yum -y install java-1.8.0
   $ sudo yum -y remove java-1.7.0-openjdk

3. 파이선 3.x 을 설치한다.

   $ sudo yum list | grep python3
   $ sudo yum -y install python36
   $ python3 -V

4. Spark 를 설치한다.

   https://spark.apache.org/downloads.html 에서 다운받아 설치한다.

   $ cd ~/
   $ mkdir spark
   $ cd spark
   $ wget http://mirror.navercorp.com/apache/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz
   $ tar xvfz spark-2.3.0-bin-hadoop2.7.tgz

   $ vi ~/.bashrc
   ......
   export SPARK_HOME=/home/ec2-user/spark/spark-2.3.0-bin-hadoop2.7
   export PATH=$SPARK_HOME/bin:$PATH
   ......
   $ source ~/.bashrc

   정상 실행되는지 확인한다.

   $ spark-shell

   정상적으로 실행되면 :quit 을 입력해 종료한다.

6. pyspark 를 설치한다.

   pip3 를 설치한다.

   $ sudo yum -y install python36-setuptools
   $ sudo easy_install-3.6 pip

   pyspark 를 설치한다.

   $ sudo pip-3.6 install pyspark

7. jupyter nootbook 을 설치한다.

   $ sudo pip-3.6 install jupyter

   $ vi ~/.bashrc
   ......
   export PYSPARK_PYTHON=/usr/bin/python3
   export PYSPARK_DRIVER_PYTHON=ipython3
   export PYSPARK_DRIVER_PYTHON_OPTS="notebook"
   ......
   $ source ~/.bashrc

   정상적으로 실행되는지 확인한다.

   $ jupyter notebook

8. 노트북을 외부에서 접속 가능하도록 수정한다.

   설정파일을 수정한다.

   $ jupyter notebook --generate-config

   아래 명령으로 외부에서 접속 가능하도록 수정한다.
   (!!!!!! 아래 명령은 모든 아이피에서 접속가능하도록 설정한다. 방화벽 등 기타 다른 방법으로 접속을 제한해야 한다. !!!!!!)

   $ vi /home/ec2-user/.jupyter/jupyter_notebook_config.py
   ......
   c.NotebookApp.ip = '*'
   ......
   c.NotebookApp.port = 8888
   ......

   AWS 콘솔에서 SparkMaster 인스턴스 보안그룹에 접속할 아이피(PC 아이피) 및 포트(8888) 를 열어준다.

   아래 명령을 실행한다.

   $ jupyter notebook

   브라우저에서 확인한다.

   http://<PC 아이피>:8888/?token=91fb1aa750ade184a9c04335f20694cfb37a2a971bd3a

9. 아래 명령으로 노트북을 백그라운드에서 실행할 수 있다.

   $ nohup jupyter notebook &
   $ cat nohup.out

10. 정상작동을 확인한다.

   노트북에 아래 코드를 입력하고 실행한다.
   에러없이 실행되면, pyspark 가 정상작동함을 확인할 수 있다.

   --------------------------------------------------------------------------------
   import pyspark
   sc = pyspark.SparkContext.getOrCreate()
   sc.stop()
   --------------------------------------------------------------------------------

11. AWS 인증키를 등록해준다.

   AWS 인증키를 환경설정에 추가한다.

   $ vi ~/.bashrc
   ......
   export AWS_ACCESS_KEY_ID=액세스키
   export AWS_SECRET_ACCESS_KEY=보안액세스키
   export AWS_DEFAULT_REGION=ap-northeast-2
   ......
   $ source ~/.bashrc