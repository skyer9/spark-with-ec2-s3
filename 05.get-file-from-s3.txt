
Spark 에서 AWS S3 데이타 가져오기



pyspark 를 이용해 S3 에 저장되어 있는 데이타를 가져온다.

1. 스파크 클러스터를 생성한다.

   $ flintrock launch spark
   $ flintrock login spark

2. 파이선 3.x 을 설치한다.

   <SparkMaster> $ sudo yum -y install python36
   <SparkMaster> $ python3 -V

3. pyspark 를 설치한다.

   pip3 를 설치한다.

   <SparkMaster> $ sudo yum -y install python36-setuptools
   <SparkMaster> $ sudo easy_install-3.6 pip

   pyspark 를 설치한다.

   <SparkMaster> $ sudo pip-3.6 install pyspark

4. 스파크 클러스터에 인증키를 등록해준다.

   AWS 인증키를 환경설정에 추가한다.

   <SparkMaster> $ vi ~/.bashrc
   ......
   export AWS_ACCESS_KEY_ID=액세스키
   export AWS_SECRET_ACCESS_KEY=보안액세스키
   export AWS_DEFAULT_REGION=ap-northeast-2
   ......

   <SparkMaster> $ source ~/.bashrc

5. s3 에 접속한다.

   <SparkMaster> $ vi test.py
   --------------------------------------------------------------------------------
   # -*- coding: utf-8 -*-
   import pyspark

   sc = pyspark.SparkContext.getOrCreate()
   sc.setSystemProperty("com.amazonaws.services.s3.enableV4", "true")

   hadoopConf = sc._jsc.hadoopConfiguration()
   hadoopConf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
   hadoopConf.set("fs.s3a.endpoint", 's3.ap-northeast-2.amazonaws.com')

   # 확장자는 csv, gz 등이 가능하다.
   df = sc.textFile("s3a://버킷이름/파일이름")

   print('\n\n------------------------------------------------\n\n')
   print(df.take(10))
   print('\n\n------------------------------------------------\n\n')

   sc.stop()
   --------------------------------------------------------------------------------

   <SparkMaster> $ spark-submit test.py

6. 스파크 클러스터를 종료 및 삭제한다.

   <SparkMaster> $ exit
   $ flintrock destroy spark
